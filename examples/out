Using custom data configuration default
Reusing dataset imagenet-1k (/data/home/rvarm1/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/a1e9bfc56c3a7350165007d1176b15e9128fcaf9ab972147840529aed3ae52bc)
Using custom data configuration default
Reusing dataset imagenet-1k (/data/home/rvarm1/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/a1e9bfc56c3a7350165007d1176b15e9128fcaf9ab972147840529aed3ae52bc)
Reusing dataset wikitext (/data/home/rvarm1/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
Reusing dataset wikitext (/data/home/rvarm1/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
Reusing dataset red_caps (/data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be)
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-7d97847d5d2abc3e.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-b47bf6120dd5de9c.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-d649afaec7f4d7a0.arrow
Reusing dataset red_caps (/data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be)
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-7d97847d5d2abc3e.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-b47bf6120dd5de9c.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-d649afaec7f4d7a0.arrow
Using FSDP
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Using custom data configuration default
Reusing dataset imagenet-1k (/data/home/rvarm1/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/a1e9bfc56c3a7350165007d1176b15e9128fcaf9ab972147840529aed3ae52bc)
Using custom data configuration default
Reusing dataset imagenet-1k (/data/home/rvarm1/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/a1e9bfc56c3a7350165007d1176b15e9128fcaf9ab972147840529aed3ae52bc)
Reusing dataset wikitext (/data/home/rvarm1/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
Reusing dataset wikitext (/data/home/rvarm1/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
Reusing dataset red_caps (/data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be)
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-7d97847d5d2abc3e.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-b47bf6120dd5de9c.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-d649afaec7f4d7a0.arrow
Reusing dataset red_caps (/data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be)
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-7d97847d5d2abc3e.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-b47bf6120dd5de9c.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-d649afaec7f4d7a0.arrow
Using custom data configuration default
Using custom data configuration default
Reusing dataset imagenet-1k (/data/home/rvarm1/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/a1e9bfc56c3a7350165007d1176b15e9128fcaf9ab972147840529aed3ae52bc)
Reusing dataset imagenet-1k (/data/home/rvarm1/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/a1e9bfc56c3a7350165007d1176b15e9128fcaf9ab972147840529aed3ae52bc)
Using custom data configuration default
Using custom data configuration default
Reusing dataset imagenet-1k (/data/home/rvarm1/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/a1e9bfc56c3a7350165007d1176b15e9128fcaf9ab972147840529aed3ae52bc)
Reusing dataset imagenet-1k (/data/home/rvarm1/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/a1e9bfc56c3a7350165007d1176b15e9128fcaf9ab972147840529aed3ae52bc)
Reusing dataset wikitext (/data/home/rvarm1/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
Reusing dataset wikitext (/data/home/rvarm1/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
Using custom data configuration default
Reusing dataset imagenet-1k (/data/home/rvarm1/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/a1e9bfc56c3a7350165007d1176b15e9128fcaf9ab972147840529aed3ae52bc)
Reusing dataset wikitext (/data/home/rvarm1/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
Reusing dataset wikitext (/data/home/rvarm1/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
Using custom data configuration default
Reusing dataset imagenet-1k (/data/home/rvarm1/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/a1e9bfc56c3a7350165007d1176b15e9128fcaf9ab972147840529aed3ae52bc)
Reusing dataset wikitext (/data/home/rvarm1/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
Reusing dataset red_caps (/data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be)
Reusing dataset red_caps (/data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be)
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-7d97847d5d2abc3e.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-7d97847d5d2abc3e.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-b47bf6120dd5de9c.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-b47bf6120dd5de9c.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-d649afaec7f4d7a0.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-d649afaec7f4d7a0.arrow
Reusing dataset wikitext (/data/home/rvarm1/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
Reusing dataset red_caps (/data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be)
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-7d97847d5d2abc3e.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-b47bf6120dd5de9c.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-d649afaec7f4d7a0.arrow
Reusing dataset red_caps (/data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be)
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-7d97847d5d2abc3e.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-b47bf6120dd5de9c.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-d649afaec7f4d7a0.arrow
Reusing dataset red_caps (/data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be)
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-7d97847d5d2abc3e.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-b47bf6120dd5de9c.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-d649afaec7f4d7a0.arrow
Reusing dataset red_caps (/data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be)
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-7d97847d5d2abc3e.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-b47bf6120dd5de9c.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-d649afaec7f4d7a0.arrow
Using custom data configuration default
Reusing dataset imagenet-1k (/data/home/rvarm1/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/a1e9bfc56c3a7350165007d1176b15e9128fcaf9ab972147840529aed3ae52bc)
Using custom data configuration default
Reusing dataset imagenet-1k (/data/home/rvarm1/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/a1e9bfc56c3a7350165007d1176b15e9128fcaf9ab972147840529aed3ae52bc)
Reusing dataset wikitext (/data/home/rvarm1/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
Reusing dataset wikitext (/data/home/rvarm1/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
Reusing dataset red_caps (/data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be)
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-7d97847d5d2abc3e.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-b47bf6120dd5de9c.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-d649afaec7f4d7a0.arrow
Reusing dataset red_caps (/data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be)
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-7d97847d5d2abc3e.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-b47bf6120dd5de9c.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-d649afaec7f4d7a0.arrow
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8
Using custom data configuration default
Reusing dataset imagenet-1k (/data/home/rvarm1/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/a1e9bfc56c3a7350165007d1176b15e9128fcaf9ab972147840529aed3ae52bc)
Using custom data configuration default
Reusing dataset imagenet-1k (/data/home/rvarm1/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/a1e9bfc56c3a7350165007d1176b15e9128fcaf9ab972147840529aed3ae52bc)
Reusing dataset wikitext (/data/home/rvarm1/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
Reusing dataset wikitext (/data/home/rvarm1/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
Reusing dataset red_caps (/data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be)
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-7d97847d5d2abc3e.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-b47bf6120dd5de9c.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-d649afaec7f4d7a0.arrow
Reusing dataset red_caps (/data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be)
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-7d97847d5d2abc3e.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-b47bf6120dd5de9c.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-d649afaec7f4d7a0.arrow
Using custom data configuration default
Reusing dataset imagenet-1k (/data/home/rvarm1/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/a1e9bfc56c3a7350165007d1176b15e9128fcaf9ab972147840529aed3ae52bc)
Using custom data configuration default
Reusing dataset imagenet-1k (/data/home/rvarm1/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/a1e9bfc56c3a7350165007d1176b15e9128fcaf9ab972147840529aed3ae52bc)
Reusing dataset wikitext (/data/home/rvarm1/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
Reusing dataset wikitext (/data/home/rvarm1/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
Reusing dataset red_caps (/data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be)
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-7d97847d5d2abc3e.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-b47bf6120dd5de9c.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-d649afaec7f4d7a0.arrow
Reusing dataset red_caps (/data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be)
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-7d97847d5d2abc3e.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-b47bf6120dd5de9c.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-d649afaec7f4d7a0.arrow
Using FSDP
Using FSDP
Using FSDP
Using FSDP
Using FSDP
Using FSDP
Using FSDP
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8
Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8
Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8
Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8
Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 8 processes
----------------------------------------------------------------------------------------------------

Using custom data configuration default
Using custom data configuration default
Using custom data configuration default
Using custom data configuration default
Reusing dataset imagenet-1k (/data/home/rvarm1/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/a1e9bfc56c3a7350165007d1176b15e9128fcaf9ab972147840529aed3ae52bc)
Using custom data configuration default
Using custom data configuration default
Using custom data configuration default
Using custom data configuration default
Reusing dataset imagenet-1k (/data/home/rvarm1/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/a1e9bfc56c3a7350165007d1176b15e9128fcaf9ab972147840529aed3ae52bc)
Reusing dataset imagenet-1k (/data/home/rvarm1/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/a1e9bfc56c3a7350165007d1176b15e9128fcaf9ab972147840529aed3ae52bc)
Reusing dataset imagenet-1k (/data/home/rvarm1/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/a1e9bfc56c3a7350165007d1176b15e9128fcaf9ab972147840529aed3ae52bc)
Reusing dataset imagenet-1k (/data/home/rvarm1/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/a1e9bfc56c3a7350165007d1176b15e9128fcaf9ab972147840529aed3ae52bc)
Reusing dataset imagenet-1k (/data/home/rvarm1/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/a1e9bfc56c3a7350165007d1176b15e9128fcaf9ab972147840529aed3ae52bc)
Using custom data configuration default
Reusing dataset imagenet-1k (/data/home/rvarm1/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/a1e9bfc56c3a7350165007d1176b15e9128fcaf9ab972147840529aed3ae52bc)
Reusing dataset imagenet-1k (/data/home/rvarm1/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/a1e9bfc56c3a7350165007d1176b15e9128fcaf9ab972147840529aed3ae52bc)
Reusing dataset imagenet-1k (/data/home/rvarm1/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/a1e9bfc56c3a7350165007d1176b15e9128fcaf9ab972147840529aed3ae52bc)
Using custom data configuration default
Using custom data configuration default
Using custom data configuration default
Using custom data configuration default
Reusing dataset imagenet-1k (/data/home/rvarm1/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/a1e9bfc56c3a7350165007d1176b15e9128fcaf9ab972147840529aed3ae52bc)
Reusing dataset imagenet-1k (/data/home/rvarm1/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/a1e9bfc56c3a7350165007d1176b15e9128fcaf9ab972147840529aed3ae52bc)
Using custom data configuration default
Reusing dataset wikitext (/data/home/rvarm1/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
Reusing dataset imagenet-1k (/data/home/rvarm1/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/a1e9bfc56c3a7350165007d1176b15e9128fcaf9ab972147840529aed3ae52bc)
Reusing dataset imagenet-1k (/data/home/rvarm1/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/a1e9bfc56c3a7350165007d1176b15e9128fcaf9ab972147840529aed3ae52bc)
Reusing dataset imagenet-1k (/data/home/rvarm1/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/a1e9bfc56c3a7350165007d1176b15e9128fcaf9ab972147840529aed3ae52bc)
Using custom data configuration default
Using custom data configuration default
Reusing dataset imagenet-1k (/data/home/rvarm1/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/a1e9bfc56c3a7350165007d1176b15e9128fcaf9ab972147840529aed3ae52bc)
Reusing dataset wikitext (/data/home/rvarm1/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
Reusing dataset imagenet-1k (/data/home/rvarm1/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/a1e9bfc56c3a7350165007d1176b15e9128fcaf9ab972147840529aed3ae52bc)
Reusing dataset wikitext (/data/home/rvarm1/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
Reusing dataset wikitext (/data/home/rvarm1/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
Reusing dataset wikitext (/data/home/rvarm1/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
Reusing dataset wikitext (/data/home/rvarm1/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
Reusing dataset wikitext (/data/home/rvarm1/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
Reusing dataset wikitext (/data/home/rvarm1/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
Reusing dataset wikitext (/data/home/rvarm1/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
Reusing dataset wikitext (/data/home/rvarm1/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
Reusing dataset wikitext (/data/home/rvarm1/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
Reusing dataset wikitext (/data/home/rvarm1/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
Reusing dataset wikitext (/data/home/rvarm1/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
Reusing dataset wikitext (/data/home/rvarm1/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
Reusing dataset wikitext (/data/home/rvarm1/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
Reusing dataset red_caps (/data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be)
Reusing dataset wikitext (/data/home/rvarm1/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-7d97847d5d2abc3e.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-b47bf6120dd5de9c.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-d649afaec7f4d7a0.arrow
Reusing dataset red_caps (/data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be)
Reusing dataset red_caps (/data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be)
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-7d97847d5d2abc3e.arrow
Reusing dataset red_caps (/data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be)
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-b47bf6120dd5de9c.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-d649afaec7f4d7a0.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-7d97847d5d2abc3e.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-b47bf6120dd5de9c.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-d649afaec7f4d7a0.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-7d97847d5d2abc3e.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-b47bf6120dd5de9c.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-d649afaec7f4d7a0.arrow
Reusing dataset red_caps (/data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be)
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-7d97847d5d2abc3e.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-b47bf6120dd5de9c.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-d649afaec7f4d7a0.arrow
Reusing dataset red_caps (/data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be)
Reusing dataset red_caps (/data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be)
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-7d97847d5d2abc3e.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-b47bf6120dd5de9c.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-7d97847d5d2abc3e.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-d649afaec7f4d7a0.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-b47bf6120dd5de9c.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-d649afaec7f4d7a0.arrow
Reusing dataset red_caps (/data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be)
Reusing dataset red_caps (/data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be)
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-7d97847d5d2abc3e.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-b47bf6120dd5de9c.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-d649afaec7f4d7a0.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-7d97847d5d2abc3e.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-b47bf6120dd5de9c.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-d649afaec7f4d7a0.arrow
Reusing dataset red_caps (/data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be)
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-7d97847d5d2abc3e.arrow
Reusing dataset red_caps (/data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be)
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-b47bf6120dd5de9c.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-d649afaec7f4d7a0.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-7d97847d5d2abc3e.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-b47bf6120dd5de9c.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-d649afaec7f4d7a0.arrow
Reusing dataset red_caps (/data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be)
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-7d97847d5d2abc3e.arrow
Reusing dataset red_caps (/data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be)
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-b47bf6120dd5de9c.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-d649afaec7f4d7a0.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-7d97847d5d2abc3e.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-b47bf6120dd5de9c.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-d649afaec7f4d7a0.arrow
Reusing dataset red_caps (/data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be)
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-7d97847d5d2abc3e.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-b47bf6120dd5de9c.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-d649afaec7f4d7a0.arrow
Reusing dataset red_caps (/data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be)
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-7d97847d5d2abc3e.arrow
Reusing dataset red_caps (/data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be)
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-b47bf6120dd5de9c.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-d649afaec7f4d7a0.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-7d97847d5d2abc3e.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-b47bf6120dd5de9c.arrow
Loading cached processed dataset at /data/home/rvarm1/.cache/huggingface/datasets/red_caps/jellyfish/1.0.0/d0d70a901e22f5e3b9a7af1f96f31c6243589705a5ab782b9ac69fcf727d97be/cache-d649afaec7f4d7a0.arrow
My fsdp model  FullyShardedDataParallel(
  (_fsdp_wrapped_module): FlattenParamsWrapper(
    (_fpw_module): FLAVAForPreTraining(
      (model): FLAVAModel(
        (image_encoder): ImageTransformer(
          (embeddings): ImageEmbeddings(
            (patch_embeddings): PatchEmbeddings(
              (projection): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): FLAVATransformerEncoder(
            (layer): ModuleList(
              (0): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (1): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (2): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (3): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (4): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (5): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (6): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (7): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (8): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (9): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (10): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (11): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (12): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (13): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (14): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (15): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (16): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (17): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (18): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (19): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (20): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (21): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (22): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (23): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (24): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (25): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (26): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (27): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (28): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (29): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (30): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (31): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
            )
          )
          (layernorm): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
          (pooler): Pooler(
            (dense): Linear(in_features=1280, out_features=1280, bias=True)
            (activation): Tanh()
          )
        )
        (text_encoder): TextTransformer(
          (embeddings): TextEmbeddings(
            (word_embeddings): Embedding(30522, 1280, padding_idx=0)
            (position_embeddings): Embedding(512, 1280)
            (token_type_embeddings): Embedding(2, 1280)
            (LayerNorm): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): FLAVATransformerEncoder(
            (layer): ModuleList(
              (0): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (1): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (2): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (3): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (4): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (5): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (6): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (7): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (8): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (9): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (10): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (11): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (12): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (13): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (14): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (15): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (16): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (17): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (18): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (19): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (20): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (21): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (22): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (23): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (24): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (25): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (26): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (27): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (28): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (29): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (30): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (31): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
            )
          )
          (layernorm): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
          (pooler): Pooler(
            (dense): Linear(in_features=1280, out_features=1280, bias=True)
            (activation): Tanh()
          )
        )
        (mm_encoder): FLAVATransformerWithoutEmbeddings(
          (encoder): FLAVATransformerEncoder(
            (layer): ModuleList(
              (0): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (1): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (2): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (3): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (4): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (5): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (6): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (7): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (8): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (9): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (10): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (11): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (12): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (13): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (14): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (15): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (16): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (17): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (18): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (19): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (20): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (21): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (22): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
              (23): FullyShardedDataParallel(
                (_fsdp_wrapped_module): FlattenParamsWrapper(
                  (_fpw_module): FLAVATransformerLayer(
                    (attention): MultiHeadAttention(
                      (query): Linear(in_features=1280, out_features=1280, bias=True)
                      (key): Linear(in_features=1280, out_features=1280, bias=True)
                      (value): Linear(in_features=1280, out_features=1280, bias=True)
                      (output): Linear(in_features=1280, out_features=1280, bias=True)
                      (attn): SelfAttention()
                    )
                    (attention_dropout): Dropout(p=0.0, inplace=False)
                    (intermediate): Linear(in_features=1280, out_features=5120, bias=True)
                    (output): Linear(in_features=5120, out_features=1280, bias=True)
                    (ffn_dropout): Dropout(p=0.0, inplace=False)
                    (layernorm_before): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                    (layernorm_after): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
                  )
                )
              )
            )
          )
          (layernorm): Fp32LayerNorm((1280,), eps=1e-12, elementwise_affine=True)
          (pooler): Pooler(
            (dense): Linear(in_features=1280, out_features=1280, bias=True)
            (activation): Tanh()
          )
        )
        (image_to_mm_projection): Linear(in_features=1280, out_features=1280, bias=True)
        (text_to_mm_projection): Linear(in_features=1280, out_features=1280, bias=True)
      )
      (image_codebook): DalleVAEEncoder(
        (encoder): DalleEncoder(
          (blocks): Sequential(
            (input): DalleConv2d()
            (group_1): Sequential(
              (block_1): DalleEncoderBlock(
                (id_path): Identity()
                (res_path): Sequential(
                  (relu_1): ReLU()
                  (conv_1): DalleConv2d()
                  (relu_2): ReLU()
                  (conv_2): DalleConv2d()
                  (relu_3): ReLU()
                  (conv_3): DalleConv2d()
                  (relu_4): ReLU()
                  (conv_4): DalleConv2d()
                )
              )
              (block_2): DalleEncoderBlock(
                (id_path): Identity()
                (res_path): Sequential(
                  (relu_1): ReLU()
                  (conv_1): DalleConv2d()
                  (relu_2): ReLU()
                  (conv_2): DalleConv2d()
                  (relu_3): ReLU()
                  (conv_3): DalleConv2d()
                  (relu_4): ReLU()
                  (conv_4): DalleConv2d()
                )
              )
              (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
            )
            (group_2): Sequential(
              (block_1): DalleEncoderBlock(
                (id_path): DalleConv2d()
                (res_path): Sequential(
                  (relu_1): ReLU()
                  (conv_1): DalleConv2d()
                  (relu_2): ReLU()
                  (conv_2): DalleConv2d()
                  (relu_3): ReLU()
                  (conv_3): DalleConv2d()
                  (relu_4): ReLU()
                  (conv_4): DalleConv2d()
                )
              )
              (block_2): DalleEncoderBlock(
                (id_path): Identity()
                (res_path): Sequential(
                  (relu_1): ReLU()
                  (conv_1): DalleConv2d()
                  (relu_2): ReLU()
                  (conv_2): DalleConv2d()
                  (relu_3): ReLU()
                  (conv_3): DalleConv2d()
                  (relu_4): ReLU()
                  (conv_4): DalleConv2d()
                )
              )
              (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
            )
            (group_3): Sequential(
              (block_1): DalleEncoderBlock(
                (id_path): DalleConv2d()
                (res_path): Sequential(
                  (relu_1): ReLU()
                  (conv_1): DalleConv2d()
                  (relu_2): ReLU()
                  (conv_2): DalleConv2d()
                  (relu_3): ReLU()
                  (conv_3): DalleConv2d()
                  (relu_4): ReLU()
                  (conv_4): DalleConv2d()
                )
              )
              (block_2): DalleEncoderBlock(
                (id_path): Identity()
                (res_path): Sequential(
                  (relu_1): ReLU()
                  (conv_1): DalleConv2d()
                  (relu_2): ReLU()
                  (conv_2): DalleConv2d()
                  (relu_3): ReLU()
                  (conv_3): DalleConv2d()
                  (relu_4): ReLU()
                  (conv_4): DalleConv2d()
                )
              )
              (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
            )
            (group_4): Sequential(
              (block_1): DalleEncoderBlock(
                (id_path): DalleConv2d()
                (res_path): Sequential(
                  (relu_1): ReLU()
                  (conv_1): DalleConv2d()
                  (relu_2): ReLU()
                  (conv_2): DalleConv2d()
                  (relu_3): ReLU()
                  (conv_3): DalleConv2d()
                  (relu_4): ReLU()
                  (conv_4): DalleConv2d()
                )
              )
              (block_2): DalleEncoderBlock(
                (id_path): Identity()
                (res_path): Sequential(
                  (relu_1): ReLU()
                  (conv_1): DalleConv2d()
                  (relu_2): ReLU()
                  (conv_2): DalleConv2d()
                  (relu_3): ReLU()
                  (conv_3): DalleConv2d()
                  (relu_4): ReLU()
                  (conv_4): DalleConv2d()
                )
              )
            )
            (output): Sequential(
              (relu): ReLU()
              (conv): DalleConv2d()
            )
          )
        )
      )
      (loss): FLAVAPretrainingLoss(
        (contrastive_loss): FLAVAGlobalContrastiveLoss(
          (image_projection): Linear(in_features=768, out_features=768, bias=True)
          (text_projection): Linear(in_features=768, out_features=768, bias=True)
        )
        (mlm_loss): MaskedPredictionLoss(
          (cls): MaskedPredictionHead(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (layer_norm): Fp32LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (decoder): Linear(in_features=768, out_features=30522, bias=True)
          )
          (ce_loss): CrossEntropyLoss()
        )
        (mim_loss): MaskedPredictionLoss(
          (cls): MaskedPredictionHead(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (layer_norm): Fp32LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (decoder): Linear(in_features=768, out_features=8192, bias=True)
          )
          (ce_loss): CrossEntropyLoss()
        )
        (mmm_loss): ModuleDict(
          (mlm): MaskedPredictionLoss(
            (cls): MaskedPredictionHead(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (layer_norm): Fp32LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (decoder): Linear(in_features=768, out_features=30522, bias=True)
            )
            (ce_loss): CrossEntropyLoss()
          )
          (mim): MaskedPredictionLoss(
            (cls): MaskedPredictionHead(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (layer_norm): Fp32LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (decoder): Linear(in_features=768, out_features=8192, bias=True)
            )
            (ce_loss): CrossEntropyLoss()
          )
        )
        (itm_loss): ITMLoss(
          (pooler): Pooler(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (activation): Tanh()
          )
          (cls): TwoWayHead(
            (seq_relationship): Linear(in_features=768, out_features=2, bias=True)
          )
          (ce_loss): CrossEntropyLoss()
        )
      )
    )
  )
)
/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:616: UserWarning: Checkpoint directory /fsx/users/rvarm1/rvarm1/repos/multimodal/examples exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]

  | Name  | Type                     | Params
---------------------------------------------------
0 | model | FullyShardedDataParallel | 237 M 
---------------------------------------------------
237 M     Trainable params
0         Non-trainable params
237 M     Total params
949.130   Total estimated model params size (MB)
Training: 0it [00:00, ?it/s]Training: 0it [00:00, ?it/s]Epoch 0: : 0it [00:00, ?it/s]/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/parallel/scatter_gather.py:9: UserWarning: is_namedtuple is deprecated, please use the python checks instead
  warnings.warn("is_namedtuple is deprecated, please use the python checks instead")
DDPFullyShardedNativeStrategy: tearing down strategy...
/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/parallel/scatter_gather.py:9: UserWarning: is_namedtuple is deprecated, please use the python checks instead
  warnings.warn("is_namedtuple is deprecated, please use the python checks instead")
Traceback (most recent call last):
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/examples/flava/train.py", line 121, in <module>
    main()
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/examples/flava/train.py", line 116, in main
    trainer.fit(model, datamodule=datamodule, ckpt_path=ckpt_path)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 701, in fit
    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 654, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 741, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1166, in _run
    results = self._run_stage()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1252, in _run_stage
    return self._run_train()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1282, in _run_train
    self.fit_loop.run()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 203, in advance
    batch_output = self.batch_loop.run(kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 87, in advance
    outputs = self.optimizer_loop.run(optimizers, kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 201, in advance
    result = self._run_optimization(kwargs, self._optimizers[self.optim_progress.optimizer_position])
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 248, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, kwargs.get("batch_idx", 0), closure)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 367, in _optimizer_step
    using_lbfgs=is_lbfgs,
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1549, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/core/module.py", line 1666, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 152, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/optim/optimizer.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/optim/adamw.py", line 120, in step
    loss = closure()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 137, in _wrap_closure
    closure_result = closure()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 132, in closure
    step_output = self._step_fn()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 407, in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *kwargs.values())
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1703, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/strategies/strategy.py", line 333, in training_step
    return self.model.training_step(*args, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/examples/flava/model.py", line 78, in training_step
    output = self._step(batch, batch_idx)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/examples/flava/model.py", line 120, in _step
    required_embedding=required_embedding,
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 2432, in forward
    outputs = self._fsdp_wrapped_module(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/distributed/fsdp/flatten_params_wrapper.py", line 156, in forward
    return self.module(*inputs, **kwinputs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/torchmultimodal/models/flava/flava_model.py", line 458, in forward
    mlm_labels=mlm_labels,
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/torchmultimodal/modules/losses/flava.py", line 419, in forward
    image_masked_sequence[:, start_index:, :], mim_labels
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/torchmultimodal/modules/losses/flava.py", line 219, in forward
    prediction = self.cls(sequence_output)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/torchmultimodal/modules/losses/flava.py", line 175, in forward
    hidden_states = self.dense(hidden_states)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (150x1280 and 768x768)
/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/parallel/scatter_gather.py:9: UserWarning: is_namedtuple is deprecated, please use the python checks instead
  warnings.warn("is_namedtuple is deprecated, please use the python checks instead")
Traceback (most recent call last):
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/examples/flava/train.py", line 121, in <module>
    main()
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/examples/flava/train.py", line 116, in main
    trainer.fit(model, datamodule=datamodule, ckpt_path=ckpt_path)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 701, in fit
    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 654, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 741, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1166, in _run
    results = self._run_stage()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1252, in _run_stage
    return self._run_train()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1282, in _run_train
    self.fit_loop.run()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 203, in advance
    batch_output = self.batch_loop.run(kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 87, in advance
    outputs = self.optimizer_loop.run(optimizers, kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 201, in advance
    result = self._run_optimization(kwargs, self._optimizers[self.optim_progress.optimizer_position])
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 248, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, kwargs.get("batch_idx", 0), closure)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 367, in _optimizer_step
    using_lbfgs=is_lbfgs,
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1549, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/core/module.py", line 1666, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 152, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/optim/optimizer.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/optim/adamw.py", line 120, in step
    loss = closure()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 137, in _wrap_closure
    closure_result = closure()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 132, in closure
    step_output = self._step_fn()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 407, in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *kwargs.values())
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1703, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/strategies/strategy.py", line 333, in training_step
    return self.model.training_step(*args, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/examples/flava/model.py", line 78, in training_step
    output = self._step(batch, batch_idx)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/examples/flava/model.py", line 120, in _step
    required_embedding=required_embedding,
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 2432, in forward
    outputs = self._fsdp_wrapped_module(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/distributed/fsdp/flatten_params_wrapper.py", line 156, in forward
    return self.module(*inputs, **kwinputs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/torchmultimodal/models/flava/flava_model.py", line 458, in forward
    mlm_labels=mlm_labels,
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/torchmultimodal/modules/losses/flava.py", line 419, in forward
    image_masked_sequence[:, start_index:, :], mim_labels
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/torchmultimodal/modules/losses/flava.py", line 219, in forward
    prediction = self.cls(sequence_output)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/torchmultimodal/modules/losses/flava.py", line 175, in forward
    hidden_states = self.dense(hidden_states)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (148x1280 and 768x768)
/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/parallel/scatter_gather.py:9: UserWarning: is_namedtuple is deprecated, please use the python checks instead
  warnings.warn("is_namedtuple is deprecated, please use the python checks instead")
Traceback (most recent call last):
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/examples/flava/train.py", line 121, in <module>
    main()
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/examples/flava/train.py", line 116, in main
    trainer.fit(model, datamodule=datamodule, ckpt_path=ckpt_path)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 701, in fit
    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 654, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 741, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1166, in _run
    results = self._run_stage()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1252, in _run_stage
    return self._run_train()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1282, in _run_train
    self.fit_loop.run()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 203, in advance
    batch_output = self.batch_loop.run(kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 87, in advance
    outputs = self.optimizer_loop.run(optimizers, kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 201, in advance
    result = self._run_optimization(kwargs, self._optimizers[self.optim_progress.optimizer_position])
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 248, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, kwargs.get("batch_idx", 0), closure)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 367, in _optimizer_step
    using_lbfgs=is_lbfgs,
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1549, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/core/module.py", line 1666, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 152, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/optim/optimizer.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/optim/adamw.py", line 120, in step
    loss = closure()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 137, in _wrap_closure
    closure_result = closure()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 132, in closure
    step_output = self._step_fn()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 407, in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *kwargs.values())
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1703, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/strategies/strategy.py", line 333, in training_step
    return self.model.training_step(*args, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/examples/flava/model.py", line 78, in training_step
    output = self._step(batch, batch_idx)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/examples/flava/model.py", line 120, in _step
    required_embedding=required_embedding,
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 2432, in forward
    outputs = self._fsdp_wrapped_module(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/distributed/fsdp/flatten_params_wrapper.py", line 156, in forward
    return self.module(*inputs, **kwinputs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/torchmultimodal/models/flava/flava_model.py", line 458, in forward
    mlm_labels=mlm_labels,
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/torchmultimodal/modules/losses/flava.py", line 419, in forward
    image_masked_sequence[:, start_index:, :], mim_labels
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/torchmultimodal/modules/losses/flava.py", line 219, in forward
    prediction = self.cls(sequence_output)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/torchmultimodal/modules/losses/flava.py", line 175, in forward
    hidden_states = self.dense(hidden_states)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (150x1280 and 768x768)
/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/parallel/scatter_gather.py:9: UserWarning: is_namedtuple is deprecated, please use the python checks instead
  warnings.warn("is_namedtuple is deprecated, please use the python checks instead")
Traceback (most recent call last):
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/examples/flava/train.py", line 121, in <module>
    main()
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/examples/flava/train.py", line 116, in main
    trainer.fit(model, datamodule=datamodule, ckpt_path=ckpt_path)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 701, in fit
    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 654, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 741, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1166, in _run
    results = self._run_stage()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1252, in _run_stage
    return self._run_train()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1282, in _run_train
    self.fit_loop.run()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 203, in advance
    batch_output = self.batch_loop.run(kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 87, in advance
    outputs = self.optimizer_loop.run(optimizers, kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 201, in advance
    result = self._run_optimization(kwargs, self._optimizers[self.optim_progress.optimizer_position])
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 248, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, kwargs.get("batch_idx", 0), closure)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 367, in _optimizer_step
    using_lbfgs=is_lbfgs,
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1549, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/core/module.py", line 1666, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 152, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/optim/optimizer.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/optim/adamw.py", line 120, in step
    loss = closure()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 137, in _wrap_closure
    closure_result = closure()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 132, in closure
    step_output = self._step_fn()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 407, in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *kwargs.values())
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1703, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/strategies/strategy.py", line 333, in training_step
    return self.model.training_step(*args, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/examples/flava/model.py", line 78, in training_step
    output = self._step(batch, batch_idx)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/examples/flava/model.py", line 120, in _step
    required_embedding=required_embedding,
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 2432, in forward
    outputs = self._fsdp_wrapped_module(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/distributed/fsdp/flatten_params_wrapper.py", line 156, in forward
    return self.module(*inputs, **kwinputs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/torchmultimodal/models/flava/flava_model.py", line 458, in forward
    mlm_labels=mlm_labels,
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/torchmultimodal/modules/losses/flava.py", line 419, in forward
    image_masked_sequence[:, start_index:, :], mim_labels
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/torchmultimodal/modules/losses/flava.py", line 219, in forward
    prediction = self.cls(sequence_output)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/torchmultimodal/modules/losses/flava.py", line 175, in forward
    hidden_states = self.dense(hidden_states)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (147x1280 and 768x768)
Traceback (most recent call last):
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/examples/flava/train.py", line 121, in <module>
    main()
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/examples/flava/train.py", line 116, in main
    trainer.fit(model, datamodule=datamodule, ckpt_path=ckpt_path)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 701, in fit
    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 652, in _call_and_handle_interrupt
    return self.strategy.launcher.launch(trainer_fn, *args, trainer=self, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 93, in launch
    return function(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 741, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1166, in _run
    results = self._run_stage()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1252, in _run_stage
    return self._run_train()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1282, in _run_train
    self.fit_loop.run()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 203, in advance
    batch_output = self.batch_loop.run(kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 87, in advance
    outputs = self.optimizer_loop.run(optimizers, kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 201, in advance
    result = self._run_optimization(kwargs, self._optimizers[self.optim_progress.optimizer_position])
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 248, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, kwargs.get("batch_idx", 0), closure)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 367, in _optimizer_step
    using_lbfgs=is_lbfgs,
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1549, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/core/module.py", line 1666, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 152, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/optim/optimizer.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/optim/adamw.py", line 120, in step
    loss = closure()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 137, in _wrap_closure
    closure_result = closure()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 132, in closure
    step_output = self._step_fn()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 407, in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *kwargs.values())
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1703, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/strategies/strategy.py", line 333, in training_step
    return self.model.training_step(*args, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/examples/flava/model.py", line 78, in training_step
    output = self._step(batch, batch_idx)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/examples/flava/model.py", line 120, in _step
    required_embedding=required_embedding,
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 2432, in forward
    outputs = self._fsdp_wrapped_module(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/distributed/fsdp/flatten_params_wrapper.py", line 156, in forward
    return self.module(*inputs, **kwinputs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/torchmultimodal/models/flava/flava_model.py", line 458, in forward
    mlm_labels=mlm_labels,
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/torchmultimodal/modules/losses/flava.py", line 419, in forward
    image_masked_sequence[:, start_index:, :], mim_labels
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/torchmultimodal/modules/losses/flava.py", line 219, in forward
    prediction = self.cls(sequence_output)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/torchmultimodal/modules/losses/flava.py", line 175, in forward
    hidden_states = self.dense(hidden_states)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (148x1280 and 768x768)
/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/parallel/scatter_gather.py:9: UserWarning: is_namedtuple is deprecated, please use the python checks instead
  warnings.warn("is_namedtuple is deprecated, please use the python checks instead")
Traceback (most recent call last):
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/examples/flava/train.py", line 121, in <module>
    main()
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/examples/flava/train.py", line 116, in main
    trainer.fit(model, datamodule=datamodule, ckpt_path=ckpt_path)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 701, in fit
    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 654, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 741, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1166, in _run
    results = self._run_stage()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1252, in _run_stage
    return self._run_train()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1282, in _run_train
    self.fit_loop.run()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 203, in advance
    batch_output = self.batch_loop.run(kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 87, in advance
    outputs = self.optimizer_loop.run(optimizers, kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 201, in advance
    result = self._run_optimization(kwargs, self._optimizers[self.optim_progress.optimizer_position])
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 248, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, kwargs.get("batch_idx", 0), closure)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 367, in _optimizer_step
    using_lbfgs=is_lbfgs,
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1549, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/core/module.py", line 1666, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 152, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/optim/optimizer.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/optim/adamw.py", line 120, in step
    loss = closure()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 137, in _wrap_closure
    closure_result = closure()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 132, in closure
    step_output = self._step_fn()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 407, in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *kwargs.values())
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1703, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/strategies/strategy.py", line 333, in training_step
    return self.model.training_step(*args, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/examples/flava/model.py", line 78, in training_step
    output = self._step(batch, batch_idx)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/examples/flava/model.py", line 120, in _step
    required_embedding=required_embedding,
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 2432, in forward
    outputs = self._fsdp_wrapped_module(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/distributed/fsdp/flatten_params_wrapper.py", line 156, in forward
    return self.module(*inputs, **kwinputs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/torchmultimodal/models/flava/flava_model.py", line 458, in forward
    mlm_labels=mlm_labels,
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/torchmultimodal/modules/losses/flava.py", line 419, in forward
    image_masked_sequence[:, start_index:, :], mim_labels
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/torchmultimodal/modules/losses/flava.py", line 219, in forward
    prediction = self.cls(sequence_output)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/torchmultimodal/modules/losses/flava.py", line 175, in forward
    hidden_states = self.dense(hidden_states)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (148x1280 and 768x768)
/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/parallel/scatter_gather.py:9: UserWarning: is_namedtuple is deprecated, please use the python checks instead
  warnings.warn("is_namedtuple is deprecated, please use the python checks instead")
Traceback (most recent call last):
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/examples/flava/train.py", line 121, in <module>
    main()
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/examples/flava/train.py", line 116, in main
    trainer.fit(model, datamodule=datamodule, ckpt_path=ckpt_path)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 701, in fit
    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 654, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 741, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1166, in _run
    results = self._run_stage()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1252, in _run_stage
    return self._run_train()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1282, in _run_train
    self.fit_loop.run()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 203, in advance
    batch_output = self.batch_loop.run(kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 87, in advance
    outputs = self.optimizer_loop.run(optimizers, kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 201, in advance
    result = self._run_optimization(kwargs, self._optimizers[self.optim_progress.optimizer_position])
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 248, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, kwargs.get("batch_idx", 0), closure)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 367, in _optimizer_step
    using_lbfgs=is_lbfgs,
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1549, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/core/module.py", line 1666, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 152, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/optim/optimizer.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/optim/adamw.py", line 120, in step
    loss = closure()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 137, in _wrap_closure
    closure_result = closure()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 132, in closure
    step_output = self._step_fn()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 407, in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *kwargs.values())
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1703, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/strategies/strategy.py", line 333, in training_step
    return self.model.training_step(*args, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/examples/flava/model.py", line 78, in training_step
    output = self._step(batch, batch_idx)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/examples/flava/model.py", line 120, in _step
    required_embedding=required_embedding,
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 2432, in forward
    outputs = self._fsdp_wrapped_module(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/distributed/fsdp/flatten_params_wrapper.py", line 156, in forward
    return self.module(*inputs, **kwinputs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/torchmultimodal/models/flava/flava_model.py", line 458, in forward
    mlm_labels=mlm_labels,
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/torchmultimodal/modules/losses/flava.py", line 419, in forward
    image_masked_sequence[:, start_index:, :], mim_labels
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/torchmultimodal/modules/losses/flava.py", line 219, in forward
    prediction = self.cls(sequence_output)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/torchmultimodal/modules/losses/flava.py", line 175, in forward
    hidden_states = self.dense(hidden_states)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (150x1280 and 768x768)
/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/parallel/scatter_gather.py:9: UserWarning: is_namedtuple is deprecated, please use the python checks instead
  warnings.warn("is_namedtuple is deprecated, please use the python checks instead")
Traceback (most recent call last):
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/examples/flava/train.py", line 121, in <module>
    main()
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/examples/flava/train.py", line 116, in main
    trainer.fit(model, datamodule=datamodule, ckpt_path=ckpt_path)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 701, in fit
    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 654, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 741, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1166, in _run
    results = self._run_stage()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1252, in _run_stage
    return self._run_train()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1282, in _run_train
    self.fit_loop.run()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 203, in advance
    batch_output = self.batch_loop.run(kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 87, in advance
    outputs = self.optimizer_loop.run(optimizers, kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 201, in advance
    result = self._run_optimization(kwargs, self._optimizers[self.optim_progress.optimizer_position])
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 248, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, kwargs.get("batch_idx", 0), closure)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 367, in _optimizer_step
    using_lbfgs=is_lbfgs,
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1549, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/core/module.py", line 1666, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 152, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/optim/optimizer.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/optim/adamw.py", line 120, in step
    loss = closure()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 137, in _wrap_closure
    closure_result = closure()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 132, in closure
    step_output = self._step_fn()
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 407, in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *kwargs.values())
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1703, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/pytorch_lightning/strategies/strategy.py", line 333, in training_step
    return self.model.training_step(*args, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/examples/flava/model.py", line 78, in training_step
    output = self._step(batch, batch_idx)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/examples/flava/model.py", line 120, in _step
    required_embedding=required_embedding,
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 2432, in forward
    outputs = self._fsdp_wrapped_module(*args, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/distributed/fsdp/flatten_params_wrapper.py", line 156, in forward
    return self.module(*inputs, **kwinputs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/torchmultimodal/models/flava/flava_model.py", line 458, in forward
    mlm_labels=mlm_labels,
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/torchmultimodal/modules/losses/flava.py", line 419, in forward
    image_masked_sequence[:, start_index:, :], mim_labels
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/torchmultimodal/modules/losses/flava.py", line 219, in forward
    prediction = self.cls(sequence_output)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/rvarm1/repos/multimodal/torchmultimodal/modules/losses/flava.py", line 175, in forward
    hidden_states = self.dense(hidden_states)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/users/rvarm1/conda/envs/tmm/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (150x1280 and 768x768)
Epoch 0: : 0it [00:06, ?it/s]
